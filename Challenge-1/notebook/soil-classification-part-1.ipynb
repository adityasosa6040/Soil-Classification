{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":102672,"databundleVersionId":12375409,"sourceType":"competition"},{"sourceId":11946263,"sourceType":"datasetVersion","datasetId":7510176}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport shutil\n\nfolder_path = '/kaggle/working/'\n\nfor item in os.listdir(folder_path):\n    item_path = os.path.join(folder_path, item)\n    try:\n        if os.path.isfile(item_path) or os.path.islink(item_path):\n            os.unlink(item_path)  # remove file\n        elif os.path.isdir(item_path):\n            shutil.rmtree(item_path)  # remove directory and its contents\n    except Exception as e:\n        print(f'Failed to delete {item_path}. Reason: {e}')\n\nprint(f\"Contents of {folder_path} cleared.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T13:03:51.483356Z","iopub.execute_input":"2025-05-25T13:03:51.483617Z","iopub.status.idle":"2025-05-25T13:03:51.531019Z","shell.execute_reply.started":"2025-05-25T13:03:51.483590Z","shell.execute_reply":"2025-05-25T13:03:51.530448Z"}},"outputs":[{"name":"stdout","text":"Contents of /kaggle/working/ cleared.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import os\nimport zipfile\nimport pandas as pd\nimport numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, f1_score\nfrom tqdm import tqdm\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\n\nimport pandas as pd\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\nfrom torchvision import transforms\n\nimport torch\nfrom torch.utils.data import Dataset\nimport os\n\nimport kornia.augmentation as K\n\nlabel_map = {\"Alluvial soil\": 0, \"Black Soil\": 1, \"Clay soil\": 2, \"Red soil\": 3}\n\nclass SoilDataset(Dataset):\n    def __init__(self, dataframe, root_dir, transform=None):  # Accept DataFrame\n        self.data = dataframe\n        self.root_dir = root_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, index):\n        img_path = os.path.join(self.root_dir, self.data.iloc[index, 0])\n        image = Image.open(img_path).convert(\"RGB\")\n        label = label_map[self.data.iloc[index, 1]]\n        if self.transform:\n            image = self.transform(image)\n        return image, label\n\nfrom torch.utils.data import DataLoader\n\nimport torchvision.models as models\nimport torch.nn as nn\n\n# Training transforms\ntrain_transform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n#Augmentation performed on gpu for faster training\ngpu_augmentations = nn.Sequential(\n    K.RandomHorizontalFlip(p=0.5),\n    # training for angled images\n    K.RandomRotation(degrees=30),\n    #train for blurry images\n    K.RandomGaussianBlur(kernel_size=(3, 3), sigma=(0.1, 2.0), p=0.5),\n).to(\"cuda\")\n\n# Validation transforms (no augmentation)\nval_transform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\nval_normalize = nn.Sequential(\n    # K.Normalize(\n    #     mean=torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1),\n    #     std=torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1)\n    # )\n).to(\"cuda\")\n\nfrom PIL import Image\nimport os\n\ndef resize_and_centercrop224(img):\n        original_width, original_height = img.size\n        target_width, target_height = (224, 224)\n\n        # Calculate aspect ratios\n        original_aspect = original_width / original_height\n        target_aspect = target_width / target_height\n\n        if original_aspect > target_aspect:\n            # Original image is wider than target: Resize based on height\n            new_height = target_height\n            new_width = int(new_height * original_aspect)\n        else:\n            # Original image is taller than target (or same aspect): Resize based on width\n            new_width = target_width\n            new_height = int(new_width / original_aspect)\n\n        # Resize the image\n        img_resized = img.resize((new_width, new_height), Image.Resampling.LANCZOS)\n\n        # Calculate coordinates for centercropping\n        left = (new_width - target_width) / 2\n        top = (new_height - target_height) / 2\n        right = (new_width + target_width) / 2\n        bottom = (new_height + target_height) / 2\n\n        # Crop the image\n        img_cropped = img_resized.crop((left, top, right, bottom))\n\n        return img_cropped\n        \ndef preprocess_images(input_dir, output_dir, size=224):\n    os.makedirs(output_dir, exist_ok=True)\n    for filename in os.listdir(input_dir):\n        img = Image.open(os.path.join(input_dir, filename)).convert(\"RGB\")\n        img = resize_and_centercrop224(img)\n        img.save(os.path.join(output_dir, filename))\n\n#resize only one time instead of transforms.resize in every fetch cycle\npreprocess_images(\"/kaggle/input/c/soil-classification/soil_classification-2025/train\", \"train_resized\")\nprint(\"images saved\")\n\ndf = pd.read_csv(\"/kaggle/input/c/soil-classification/soil_classification-2025/train_labels.csv\")\n\n#train-test split for validation dataset\ntrain_df, val_df = train_test_split(\n    df, \n    test_size=0.2, \n    stratify=df[\"soil_type\"],\n    random_state=42\n)\n\n# Training dataset\ntrain_dataset = SoilDataset(\n    dataframe=train_df,\n    root_dir=\"train_resized\",\n    transform=train_transform\n)\n\n# Validation dataset\nval_dataset = SoilDataset(\n    dataframe=val_df,\n    root_dir=\"train_resized\",\n    transform=val_transform\n)\n\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=8, pin_memory=True, persistent_workers=True)\n# No shuffle for val\nval_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=8, pin_memory=True, persistent_workers=True)\n\nmodel = models.resnet18(pretrained=True) # for small dataset\nmodel = nn.DataParallel(model) # use both gpus\nmodel = model.to(\"cuda\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-25T13:04:02.936301Z","iopub.execute_input":"2025-05-25T13:04:02.936973Z","iopub.status.idle":"2025-05-25T13:04:17.931352Z","shell.execute_reply.started":"2025-05-25T13:04:02.936943Z","shell.execute_reply":"2025-05-25T13:04:17.930553Z"}},"outputs":[{"name":"stdout","text":"images saved\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import torch.optim as optim\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\nbest_val_loss = float(\"inf\")\npatience = 10 # early stopping after 10 worse val losses\nepochs_no_improve = 0\n\nfor epoch in range(50):\n    model.train()\n    running_train_loss = 0.0\n    \n    # Training phase\n    for images, labels in train_loader:\n        images, labels = images.to(\"cuda\"), labels.to(\"cuda\")\n        images = gpu_augmentations(images)\n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        running_train_loss += loss.item()\n    \n    # Validation phase\n    model.eval()\n    running_val_loss = 0.0\n    with torch.no_grad():\n        for images, labels in val_loader:\n            images, labels = images.to(\"cuda\"), labels.to(\"cuda\")\n            images = val_normalize(images)\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            running_val_loss += loss.item()\n    \n    # Calculate average losses\n    avg_train_loss = running_train_loss / len(train_loader)\n    avg_val_loss = running_val_loss / len(val_loader)\n    \n    print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss:.2f}, Val Loss: {avg_val_loss:.2f}\")\n    \n    # Early stopping check (based on validation loss)\n    if avg_val_loss < best_val_loss:\n        best_val_loss = avg_val_loss\n        epochs_no_improve = 0\n        #save best model parameters for generating submission.csv\n        torch.save(model.state_dict(), \"best_model.pth\")\n        print(\"saved best_model.pth\")\n    else:\n        epochs_no_improve += 1\n        if epochs_no_improve >= patience:\n            print(\"Early stopping triggered!\")\n            break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T13:04:33.969751Z","iopub.execute_input":"2025-05-25T13:04:33.970040Z","iopub.status.idle":"2025-05-25T13:08:00.670347Z","shell.execute_reply.started":"2025-05-25T13:04:33.970013Z","shell.execute_reply":"2025-05-25T13:08:00.669346Z"}},"outputs":[{"name":"stdout","text":"Epoch 1, Train Loss: 1.78, Val Loss: 6.21\nsaved best_model.pth\nEpoch 2, Train Loss: 0.62, Val Loss: 2.91\nsaved best_model.pth\nEpoch 3, Train Loss: 0.28, Val Loss: 0.30\nsaved best_model.pth\nEpoch 4, Train Loss: 0.42, Val Loss: 0.28\nsaved best_model.pth\nEpoch 5, Train Loss: 0.45, Val Loss: 0.76\nEpoch 6, Train Loss: 0.23, Val Loss: 0.22\nsaved best_model.pth\nEpoch 7, Train Loss: 0.22, Val Loss: 0.20\nsaved best_model.pth\nEpoch 8, Train Loss: 0.18, Val Loss: 0.20\nsaved best_model.pth\nEpoch 9, Train Loss: 0.15, Val Loss: 0.17\nsaved best_model.pth\nEpoch 10, Train Loss: 0.17, Val Loss: 0.17\nEpoch 11, Train Loss: 0.26, Val Loss: 0.23\nEpoch 12, Train Loss: 0.20, Val Loss: 0.28\nEpoch 13, Train Loss: 0.16, Val Loss: 0.19\nEpoch 14, Train Loss: 0.18, Val Loss: 0.23\nEpoch 15, Train Loss: 0.18, Val Loss: 0.21\nEpoch 16, Train Loss: 0.16, Val Loss: 0.34\nEpoch 17, Train Loss: 0.19, Val Loss: 0.25\nEpoch 18, Train Loss: 0.16, Val Loss: 0.25\nEpoch 19, Train Loss: 0.12, Val Loss: 0.16\nsaved best_model.pth\nEpoch 20, Train Loss: 0.13, Val Loss: 0.28\nEpoch 21, Train Loss: 0.23, Val Loss: 0.35\nEpoch 22, Train Loss: 0.18, Val Loss: 0.18\nEpoch 23, Train Loss: 0.13, Val Loss: 0.15\nsaved best_model.pth\nEpoch 24, Train Loss: 0.12, Val Loss: 0.16\nEpoch 25, Train Loss: 0.19, Val Loss: 0.17\nEpoch 26, Train Loss: 0.13, Val Loss: 0.27\nEpoch 27, Train Loss: 0.12, Val Loss: 0.26\nEpoch 28, Train Loss: 0.09, Val Loss: 0.24\nEpoch 29, Train Loss: 0.08, Val Loss: 0.20\nEpoch 30, Train Loss: 0.11, Val Loss: 0.15\nEpoch 31, Train Loss: 0.11, Val Loss: 0.17\nEpoch 32, Train Loss: 0.15, Val Loss: 0.18\nEpoch 33, Train Loss: 0.12, Val Loss: 0.14\nsaved best_model.pth\nEpoch 34, Train Loss: 0.15, Val Loss: 0.18\nEpoch 35, Train Loss: 0.14, Val Loss: 0.20\nEpoch 36, Train Loss: 0.10, Val Loss: 0.22\nEpoch 37, Train Loss: 0.08, Val Loss: 0.11\nsaved best_model.pth\nEpoch 38, Train Loss: 0.12, Val Loss: 0.22\nEpoch 39, Train Loss: 0.11, Val Loss: 0.18\nEpoch 40, Train Loss: 0.10, Val Loss: 0.21\nEpoch 41, Train Loss: 0.08, Val Loss: 0.13\nEpoch 42, Train Loss: 0.06, Val Loss: 0.17\nEpoch 43, Train Loss: 0.09, Val Loss: 0.17\nEpoch 44, Train Loss: 0.12, Val Loss: 0.59\nEpoch 45, Train Loss: 0.13, Val Loss: 0.26\nEpoch 46, Train Loss: 0.08, Val Loss: 0.18\nEpoch 47, Train Loss: 0.15, Val Loss: 0.21\nEarly stopping triggered!\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"from sklearn.metrics import f1_score\n\nmodel.load_state_dict(torch.load(\"/kaggle/input/best-model-part-1/best_model.pth\"))\nmodel.eval()\ny_true, y_pred = [], []\n\n# Validation dataset\ndataset = SoilDataset(\n    dataframe=df,\n    root_dir=\"train_resized\",\n    transform=val_transform\n)\n\nval_loader = DataLoader(dataset, batch_size=64, shuffle=False)\n\nwith torch.no_grad():\n    for images, labels in val_loader:\n        images = images.to(\"cuda\")\n        images = val_normalize(images)\n        outputs = model(images)\n        preds = torch.argmax(outputs, 1).cpu().numpy()\n        y_pred.extend(preds)\n        y_true.extend(labels.numpy())\n\nf1s = f1_score(y_true, y_pred, average=None)\nprint(\"F1 Scores for each class:\", f1s)\nprint(\"Average F1 (Macro):\", np.mean(f1s))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T13:14:04.104827Z","iopub.execute_input":"2025-05-25T13:14:04.105426Z","iopub.status.idle":"2025-05-25T13:14:07.760374Z","shell.execute_reply.started":"2025-05-25T13:14:04.105389Z","shell.execute_reply":"2025-05-25T13:14:07.759544Z"}},"outputs":[{"name":"stdout","text":"F1 Scores for each class: [0.98265896 0.98717949 0.96350365 0.99810247]\nAverage F1 (Macro): 0.9828611407863161\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"submission = []\n\nval_transform = transforms.Compose([\n    transforms.Resize(224),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\nmodel.eval()\nfor filename in sorted(os.listdir(\"/kaggle/input/c/soil-classification/soil_classification-2025/test\")):\n    img = Image.open(os.path.join(\"/kaggle/input/c/soil-classification/soil_classification-2025/test\", filename)).convert(\"RGB\")\n    img = val_normalize(val_transform(img)).unsqueeze(0).to(\"cuda\")\n    output = model(img)\n    pred = torch.argmax(output, 1).item()\n    label = list(label_map.keys())[list(label_map.values()).index(pred)]\n    submission.append((filename, label))\n\npd.DataFrame(submission, columns=[\"filename\", \"label\"]).to_csv(\"submission.csv\", index=False)\nprint(\"submission.csv generated\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T13:14:15.244809Z","iopub.execute_input":"2025-05-25T13:14:15.245346Z","iopub.status.idle":"2025-05-25T13:14:21.518305Z","shell.execute_reply.started":"2025-05-25T13:14:15.245318Z","shell.execute_reply":"2025-05-25T13:14:21.517533Z"}},"outputs":[{"name":"stdout","text":"submission.csv generated\n","output_type":"stream"}],"execution_count":8}]}